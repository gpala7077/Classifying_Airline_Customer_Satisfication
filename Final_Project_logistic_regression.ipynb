{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "X_train_s = np.load('data/X_train_s.npy' )\n",
    "X_test_s = np.load('data/X_test_s.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "feature_names = np.load('data/feature_names.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a type of regression analysis used when predicting classes as opposed to numerical values. In\n",
    "this case, its goal is to predict whether a customer is a satisfied or dissatisfied customer. In order to fine-tune the\n",
    "model, a cross-validated grid-search will be performed running various combination of tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def find_best_parameters(model,parameters, x, y):\n",
    "    gridSearch = GridSearchCV(model,parameters)\n",
    "    gridSearch.fit(x,y)\n",
    "    return gridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logRegGrid = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'fit_intercept': [True,False],\n",
    "    'class_weight': ['balanced',None],\n",
    "    'tol': [.0001, .01, 1, 10, 100],\n",
    "    'C': [.01, 1, 10, 100],\n",
    "    'l1_ratio': np.linspace(0,1,num=4),\n",
    "    'n_jobs':[-1]\n",
    "}\n",
    "\n",
    "best = find_best_parameters(LogisticRegression(),logRegGrid,X_train_s,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the best parameters from the grid search, the following model achieved a score of 87.36%. The model was balanced\n",
    "in predicting satisfied and unsatisfied customers. This is shown within the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for param in best.best_params_:\n",
    "    print('{} : {}'.format(param,best.best_params_[param]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logReg = LogisticRegression(\n",
    "    C=100,\n",
    "    tol=.01,\n",
    "    class_weight=None,\n",
    "    fit_intercept=False,\n",
    "    l1_ratio=1.0,\n",
    "    max_iter=1000,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga'\n",
    ")\n",
    "\n",
    "logReg.fit(X_train_s, y_train)\n",
    "predictions = logReg.predict(X_test_s)\n",
    "score = logReg.score(X_test_s, y_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Accuracy Score: {:.2f}%'.format(score*100), size = 15);\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "rf_roc = plot_roc_curve(logReg, X_test_s, y_test)\n",
    "plt.title(\"ROC\")\n",
    "plt.show()\n",
    "metrics.plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to improve accuracy and reduce the complexity of the model, feature selection could help. In this case,\n",
    "the following will systematically choose the best number of features by minimizing the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection, model_selection\n",
    "\n",
    "def select_features(training_data,target,model, percentiles, scoring='neg_mean_absolute_error',cv=5):\n",
    "    results = []\n",
    "    for i in percentiles:\n",
    "        fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=i)\n",
    "        X_train_fs = fs.fit_transform(training_data, target)\n",
    "        scores = model_selection.cross_val_score(model, X_train_fs, target, cv=cv,scoring=scoring)\n",
    "        results = np.append(results, abs(scores).mean())\n",
    "\n",
    "    optimal_percentile = np.where(results == results.min())[0]\n",
    "    print (\"Optimal percentile of features:{0}\".format(percentiles[optimal_percentile[0]]), \"\\n\")\n",
    "    optimal_num_features = int(percentiles[optimal_percentile[0]]*training_data.shape[1]/100)\n",
    "    print (\"Optimal number of features:{0}\".format(optimal_num_features), \"\\n\")\n",
    "    return optimal_percentile, results\n",
    "\n",
    "def plot_coefficients(model, n_features, feature_names):\n",
    "    plt.figure(figsize=(10,20))\n",
    "    plt.yticks(np.arange(n_features), feature_names)\n",
    "    plt.xlabel(\"Coefficient Value\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.barh(range(n_features), model, align='center')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "percentiles = range(1, 101)\n",
    "\n",
    "optimal_percentile, results = select_features(X_train_s,y_train,logReg,percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=93)\n",
    "X_train_fs = fs.fit_transform(X_train_s, y_train)\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['columns'] = feature_names[fs.get_support()]\n",
    "features['weights'] = fs.scores_[fs.get_support()]\n",
    "features = features.sort_values(by='weights')\n",
    "plot_coefficients(features['weights'], len(features['columns']), features['columns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test_fs = X_test_s[:,fs.get_support()]\n",
    "\n",
    "logReg.fit(X_train_fs, y_train)\n",
    "predictions = logReg.predict(X_test_fs)\n",
    "score = logReg.score(X_test_fs, y_test)\n",
    "\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Accuracy Score: {:.2f}%'.format(score*100), size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "selector = RFE(logReg)\n",
    "selector.fit(X_train_s,y_train)\n",
    "predictions = selector.predict(X_test_s)\n",
    "score = selector.score(X_test_s, y_test)\n",
    "\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Accuracy Score: {:.2f}%'.format(score*100), size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_rfe = selector.transform(X_train_s)\n",
    "\n",
    "logReg.fit(X_train_rfe, y_train)\n",
    "features = pd.DataFrame()\n",
    "features['columns'] = feature_names[selector.support_]\n",
    "features['weights'] = logReg.coef_[0]\n",
    "features = features.sort_values(by='weights')\n",
    "plot_coefficients(features['weights'], len(features['columns']), features['columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "variance = 0\n",
    "i = 0\n",
    "while variance < .95:\n",
    "    i += 1\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X_train_s)\n",
    "    variance = pca.explained_variance_ratio_.sum()\n",
    "    print('{} components capture {:.2f}%'.format(i,variance*100))\n",
    "\n",
    "print('To capture at least 95% of the variance, {} principal components are needed'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca_data = pca.fit_transform(X_train_s,y_train)\n",
    "pca_data_test = pca.fit_transform(X_test_s,y_test)\n",
    "logReg.fit(pca_data, y_train)\n",
    "predictions = logReg.predict(pca_data_test)\n",
    "score = logReg.score(pca_data_test, y_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Accuracy Score: {:.2f}%'.format(score*100), size = 15);\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train,X_train_rfe)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings = pd.DataFrame(loadings.T,columns=feature_names)\n",
    "loadings[loadings > .01]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
